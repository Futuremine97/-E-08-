{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EX08.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw9KIUBHS3I2",
        "outputId": "b894ce2b-cfb3-49a2-87ea-67d471df57fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 155229 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.27-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.27-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.27-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "\n",
        "vcode = getpass.getpass()\n",
        "\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p Gdrive"
      ],
      "metadata": {
        "id": "1PO404_hS7G9"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!google-drive-ocamlfuse Gdrive"
      ],
      "metadata": {
        "id": "XyelbzwFS7SR"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj3U95cqTZpN",
        "outputId": "2dd3969c-d5ce-40fe-c5f3-15cb5cff48ff"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "adc.json  Gdrive  news_summary_more.csv  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls Gdrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdveSmoFTa-2",
        "outputId": "e74b82e1-f7fa-41d3-fbd2-8fbce1f65bff"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 귤\n",
            " 잠\n",
            " 밥\n",
            "'새 폴더'\n",
            " 회로\n",
            " 공부\n",
            " 산책\n",
            " 클럽\n",
            " 먹방\n",
            " 웃음\n",
            " 노래\n",
            "'학식 고'\n",
            " 지하철\n",
            " 영화관\n",
            " 공부방\n",
            "'포트폴리오 - 영상 - 김선현 어드밴스'\n",
            "'사랑에빠진 귤이'\n",
            "'파일_000 (08db3a9c).jpeg'\n",
            "'파일_000 (0c906857).jpeg'\n",
            "'파일_000 (0f70e3fe).png'\n",
            "'파일_000 (1f01042b).jpeg'\n",
            "'파일_000 (293804dd).png'\n",
            "'파일_000 (2dfe88a0).png'\n",
            "'파일_000 (366fd6a8).jpeg'\n",
            "'파일_000 (3c61f657).png'\n",
            "'파일_000 (429974e7).jpeg'\n",
            "'파일_000 (45c0978d).jpeg'\n",
            "'파일_000 (45cee142).jpeg'\n",
            "'파일_000 (4ab2b995).jpeg'\n",
            "'파일_000 (4cc44fc9).png'\n",
            "'파일_000 (51935fd4).png'\n",
            "'파일_000 (5621fe32).jpeg'\n",
            "'파일_000 (8d3d6851).jpeg'\n",
            "'파일_000 (96a852e5).jpeg'\n",
            "'파일_000 (9b12d82a).jpeg'\n",
            "'파일_000 (bd9a9b63).jpeg'\n",
            "'파일_000 (f4810184).jpeg'\n",
            " 파일_000.jpeg\n",
            " 파일_000.png\n",
            " 파일_007.png\n",
            " 파일_008.png\n",
            " 파일_009.png\n",
            " 0117_1.desktop\n",
            " 0.pdf\n",
            " 1\n",
            " 1_001.pdf\n",
            " 10.pdf\n",
            " 11\n",
            "'대지 1-100.jpg'\n",
            "'박성훈 주간보고서1 10:22.docx'\n",
            " 11.pdf\n",
            " 11.psd\n",
            " 12\n",
            "'기초실험(1) 강의계획서_2020.hwp'\n",
            "'12 (a458bfb5).pdf'\n",
            " 12.pdf\n",
            " 13\n",
            "'1 (33b31c4c).pdf'\n",
            " 13.pdf\n",
            "'회로(1).(4장)문제풀이.b693085.박성훈.hwp'\n",
            " 14.pdf\n",
            "'1차실험 5조.hwp'\n",
            "'1626766488398-2021 창업동아리 신청서 (2) (1) (1).docx'\n",
            "'191704_오피스텔 월세 계약서.pdf'\n",
            "'미디어1 (9b86155d).mp4'\n",
            "'19p.1.3 샌드위치 정리 문제풀이 .pdf'\n",
            "'1 (ae087a70).pdf'\n",
            "'1 (d3cb2acc).pdf'\n",
            "'산재보험 자격취득자 명부 (1) (efb5d332).pdf'\n",
            "'1 (f748b403).pdf'\n",
            " 1.jpg\n",
            "'대지 1.mp4'\n",
            " 미디어1.mp4\n",
            " 1.mp4\n",
            "'박성훈 주간보고1.pages'\n",
            "'수료증 (1).pdf'\n",
            "'산재보험 자격취득자 명부 (1).pdf'\n",
            " 공무원생물1.pdf\n",
            " 전자기학1.pdf\n",
            " 1.pdf\n",
            "'대지 1.png'\n",
            " 노래1.png\n",
            " 귤2\n",
            " 응수2\n",
            " 전전기초2\n",
            "'200002_F5司 공통기술팀에서 개발표준 업무를 담딩하는 고성능 쌔信 개발팀에 SQL 작성.pdf'\n",
            " 2_002.pdf\n",
            " 20190312_170038_276.jpg\n",
            " 20190312_170045_512.jpg\n",
            " 20190312_170049_104.jpg\n",
            " 20190318_101355.jpg\n",
            " 20190320_143654.jpg\n",
            " 20190320_143717.jpg\n",
            " 20190320_143735.jpg\n",
            "'20190403_145028_756 (e58ef972).jpg'\n",
            " 20190403_145028_756.jpg\n",
            "'노트 2019. 10. 23..pdf'\n",
            "'새 파일 2020-02-06 14.11.38.pdf'\n",
            " 20200228_162813.mp4\n",
            " 20200228_164427.mp4\n",
            " 20200228_164530.mp4\n",
            "'20200305_전기기사 펼기.pdf'\n",
            "'스크린샷, 2020-04-01 오전 5.20.41.png'\n",
            " 20200517145933.pdf\n",
            " 20201103_002908.jpg\n",
            "'20201117_160747 (b801790a).jpg'\n",
            " 20201117_160747.jpg\n",
            " 20201117_162934.mp4\n",
            " 20201117_165334.jpg\n",
            "'20201117_170420 (9ca4c308).jpg'\n",
            "'20201117_170420 (b4c6324b).jpg'\n",
            " 20201117_170420.jpg\n",
            " 20201117_170548.jpg\n",
            "'20201201_151641 (d6496cb9).jpg'\n",
            " 20201201_151641.jpg\n",
            " 20201201_152302.jpg\n",
            " 20201201_153307.jpg\n",
            " 20201201_155919.jpg\n",
            " 20201201_161408.jpg\n",
            " 20201201_165955.jpg\n",
            "'20201201_172410(0).jpg'\n",
            " 20201201_172717.jpg\n",
            " 20201201_173538.jpg\n",
            " 20201201_173733.jpg\n",
            " 20201201_175003.jpg\n",
            " 20201201_175141.jpg\n",
            " 20201204_114955.jpg\n",
            " 20201204_115045.jpg\n",
            " 20201204_115134.jpg\n",
            " 20201211_181416.mp4\n",
            " 20201211_184750.mp4\n",
            " 20201211_185122.mp4\n",
            " 20201211_185535.mp4\n",
            " 20201211_191242.mp4\n",
            " 20201211_191410.mp4\n",
            " 20201211_194759.jpg\n",
            " 20201211_195249.jpg\n",
            " 20210612232955234.pdf\n",
            " 20210625172540524.pdf\n",
            "'20211112_005732 (d42f21b6).mp4'\n",
            " 20211112_005732.mp4\n",
            "'2021 실전창업교육2기 1단계 사업계획서 박성훈 01031907972 (297ffd55).pdf'\n",
            "'2021 실전창업교육2기 1단계 사업계획서 박성훈 01031907972 (b258bbc2).pdf'\n",
            "'2021 실전창업교육2기 1단계 사업계획서 박성훈 01031907972.pdf'\n",
            "'2021. 2. 2. 오후 8_32_31.mp3'\n",
            "'20220123_SGL 최적화 기본 원리.pdf'\n",
            "'20220123_S○L        활용.pdf'\n",
            "'20220123_SoL 전문가 .pdf'\n",
            "'20220123_국가공인 SQL 전문가개발자 자격검정 안내1.pdf'\n",
            "'이름 없는 노트북 (20).pdf'\n",
            "'박성훈 주간보고서2 11:04.docx'\n",
            " 22.pdf\n",
            "'전기전자기초실험2. 5조. 결과보고서 11월17일 (99cc2499).hwp'\n",
            "'전기전자기초실험2. 5조. 결과보고서 11월17일.hwp'\n",
            "'전기전자기초실험2. 5조. 결과보고서 12월 04일. (98309db3).hwp'\n",
            "'전기전자기초실험2. 5조. 결과보고서 12월 04일.hwp'\n",
            "'전기전자기초실험2. 5조. 결과보고서 12월 04일..hwp'\n",
            "'2차실험 5조.hwp'\n",
            "'2 (76ba16f8).pdf'\n",
            " 27.jpg\n",
            "'2 (ac6ab036).pdf'\n",
            " 피맥2.ai3.ai\n",
            " 2.jpg\n",
            " 2.mp4\n",
            "'가져온 파일 (2).pdf'\n",
            " 전자기학2.pdf\n",
            " 생물2.pdf\n",
            " 유2.pdf\n",
            " 2.pdf\n",
            " 뭐해최종2.png\n",
            " 3_001.pdf\n",
            " 3_002.pdf\n",
            "'3 (086d2d26).pdf'\n",
            " 31.jpg\n",
            " 33.pdf\n",
            "'3차 실험 5조.hwp'\n",
            "'3 (722faf87).pdf'\n",
            " 3.jpg\n",
            " 실험3.mp4\n",
            " 3.mp4\n",
            " 전자기학3.pdf\n",
            " 생물3.pdf\n",
            " 유3.pdf\n",
            " 3.pdf\n",
            " 일하는중3.png\n",
            " 4_001.pdf\n",
            " 4_002.pdf\n",
            "'4 (1b8945df).pdf'\n",
            " 44_001.pdf\n",
            " 44_002.pdf\n",
            " 44_003.pdf\n",
            " 44.pdf\n",
            "'4차실험 5조.hwp'\n",
            "'4 (b4de70fd).pdf'\n",
            " 4차실험.hwp\n",
            " 전자기학4.pdf\n",
            " 생물4.pdf\n",
            " 유4.pdf\n",
            " 4.pdf\n",
            " 5_001.pdf\n",
            " 5_002.pdf\n",
            "'5 (278ac527).pdf'\n",
            "'창의적공학설계 5조 3차발표3.pptx'\n",
            "'5 (74ea06f2).pdf'\n",
            "'5 (8a2e3031).pdf'\n",
            "'가져온 파일 (5).pdf'\n",
            " 전자기학5.pdf\n",
            " 생물5.pdf\n",
            " 유5.pdf\n",
            " 5.pdf\n",
            " 6_001.pdf\n",
            " 6_002.pdf\n",
            " 62.pdf\n",
            " 63.pdf\n",
            " 64.pdf\n",
            " 65.pdf\n",
            " 66.pdf\n",
            " 67.pdf\n",
            " 68.pdf\n",
            " 69.pdf\n",
            " 전자기학6.pdf\n",
            " 생6.pdf\n",
            " 6.pdf\n",
            " 7_001.pdf\n",
            " 70.pdf\n",
            "'7 (2f026fd9).pdf'\n",
            "'가져온 파일 (7).pdf'\n",
            " 전자기학7.pdf\n",
            " 생7.pdf\n",
            " 7.pdf\n",
            " 7.psd\n",
            "'8 (f817bad0).pdf'\n",
            " 8.pdf\n",
            " 9.pdf\n",
            " 9.psd\n",
            " adjustable-gamma-correction-circuit-for-tft-lcd.pdf\n",
            " ans1.pdf\n",
            " ans2.pdf\n",
            " ans3.pdf\n",
            " ans4.pdf\n",
            "'answer of tofel.pdf'\n",
            " 동서식품.avi\n",
            "'B693085 박성훈 인턴십현장실습_장학생_실습시작_보고서 (2) (1).hwp'\n",
            "'B693085 박성훈 회로이론 6장.pdf'\n",
            "'B693085 박성훈 인턴십현장실습_장학생_실습시작_보고서.. (84fed97e).hwp'\n",
            "'b693085  박성훈 과제입니다.hwp'\n",
            "'B693085 박성훈 인턴십현장실습_장학생_실습시작_보고서...hwp'\n",
            "'박성훈 b693085 성평등.pdf'\n",
            "'b693085 박성훈 전전컴 term project (866743dc).pptx'\n",
            "'b693085 박성훈 전전컴 term project.pptx'\n",
            "'ch07-에너지 저장 소자.pdf'\n",
            "'Chapter 10 반도체공학.pdf'\n",
            " Classroom\n",
            " Comic-Inker.brush\n",
            "'cs231n_2018_lecture06 (88238c23).pdf'\n",
            " cs231n_2018_lecture06.pdf\n",
            "'제목 없는 프레젠테이션.desktop'\n",
            " ㄱㅌ.desktop\n",
            " 질문방.desktop\n",
            " dg1.jpg\n",
            " dg1.pdf\n",
            " dg2.pdf\n",
            " dg3.pdf\n",
            " dg4.pdf\n",
            " dg5.pdf\n",
            " Dsp\n",
            " EX08.ipynb\n",
            "'[창업동아리] 가지급금 정산 서식_스틸시프트. (fc2848f8).hwp'\n",
            " gen1.pdf\n",
            " gen2.pdf\n",
            " gen3.pdf\n",
            " gen4.pdf\n",
            " gen5.pdf\n",
            " gen6.pdf\n",
            " gen7.pdf\n",
            " gen8.pdf\n",
            " gen9.pdf\n",
            " generalphs.pdf\n",
            "'GOMCAM 20200401_0927590540.mp4'\n",
            "'GOMCAM 20210317_1000030074.mp4'\n",
            "'GOMCAM 20210331_0959440143 (fbd16501).zip'\n",
            "'GOMCAM 20210331_0959440143.zip'\n",
            " gp2.pdf\n",
            " gp3.pdf\n",
            " gp4.pdf\n",
            " gp5.pdf\n",
            " gp6.pdf\n",
            " GREVER홀수1.pdf\n",
            " GREVER홀수2.pdf\n",
            " GREVER홀수3.pdf\n",
            " GREVER홀수4.pdf\n",
            "'[창업동아리] 가지급금 정산 서식_스틸시프트..hwp'\n",
            " HyperParameterData1.desktop\n",
            " IMG-0351.jpg\n",
            "'IMG_0640 (79204a6c).jpg'\n",
            " IMG_0640.jpg\n",
            "'IMG_0641 (a071b958).jpg'\n",
            " IMG_0641.jpg\n",
            " IMG_1683.JPG\n",
            " j2.pdf\n",
            " j3.pdf\n",
            " j4.pdf\n",
            " j5.pdf\n",
            " j6.pdf\n",
            " j7.pdf\n",
            " j8.pdf\n",
            " j9.pdf\n",
            " ㄷ.jpg\n",
            " 엄마아빠.jpg\n",
            " KakaoTalk_Image_2021-12-29-10-39-53.jpeg\n",
            " KakaoTalk_Video_2021-11-28-20-37-54.mp4\n",
            " k.png\n",
            "'lab5 오류수정 최종 TB.PNG'\n",
            " lc1.pdf\n",
            "'lc2 (514fc492).pdf'\n",
            " lc2.pdf\n",
            "'lc3 (a2dbe136).pdf'\n",
            " lc3.pdf\n",
            "'lc4 (b58d9007).pdf'\n",
            " lc4.pdf\n",
            " lc5.pdf\n",
            " lc6.pdf\n",
            "'lecture_6 : Training Neural Networks.desktop'\n",
            "'lidarsensor (010853c5).mp4'\n",
            " lidarsensor.mp4\n",
            " 제목_없는_아트워크.mp4\n",
            "'New hwp.hwp'\n",
            " Notability\n",
            "'Notability (840b2f73)'\n",
            "'Notability (dc675e59)'\n",
            "'page_ (d5a3a661).pdf'\n",
            " page_.pdf\n",
            " 수료증.pdf\n",
            " 이기적유.pdf\n",
            "'해지신청서 예시포함.pdf'\n",
            " 프레젠테이션.pdf\n",
            "'가져온 파일.pdf'\n",
            " 대학물리.pdf\n",
            " 빈출단어.pdf\n",
            " 피맥.png\n",
            " 지하철.png\n",
            " 톰크루즈.png\n",
            " 컴퓨터일과.png\n",
            " 노래방최종.png\n",
            "'라이다 임베디드 프로젝트.pptx'\n",
            "'박성훈 창업경진대회.pptx'\n",
            " q1.pdf\n",
            " q2.pdf\n",
            " q3.pdf\n",
            " q4.pdf\n",
            " q5.pdf\n",
            " 박성훈resume.01026837972.docx\n",
            " 박성훈resume.01026837972.docx\n",
            " s1.pdf\n",
            " s2.pdf\n",
            " s3.pdf\n",
            " s4.pdf\n",
            " s5.pdf\n",
            " s6.pdf\n",
            " s7.pdf\n",
            " s8.pdf\n",
            " ScanSnap\n",
            " sk2.pdf\n",
            " sk3.pdf\n",
            " sk4.pdf\n",
            " s노래.png\n",
            " sw1.pdf\n",
            " sw2.pdf\n",
            " sw3.pdf\n",
            " sw4.pdf\n",
            " sw5.pdf\n",
            " sw6.pdf\n",
            " sw7.pdf\n",
            " sw8.pdf\n",
            " t0.pdf\n",
            " t1.pdf\n",
            " t2.pdf\n",
            " tffault.pdf\n",
            "'TFLC1 (df660d90).pdf'\n",
            " tflc1.pdf\n",
            " TFLC1.pdf\n",
            "'TFLC2 (b8826155).pdf'\n",
            " TFLC2.pdf\n",
            "'TFLC3 (b538374e).pdf'\n",
            " TFLC3.pdf\n",
            "'TFLC4 (6b144248).pdf'\n",
            " TFLC4.pdf\n",
            "'TFLC5 (35c95b33).pdf'\n",
            " TFLC5.pdf\n",
            "'TFLC6 (68599b2b).pdf'\n",
            " TFLC6.pdf\n",
            " tfmr1.pdf\n",
            " tfmr2.pdf\n",
            " tfmr3.pdf\n",
            " tfmr4.pdf\n",
            " tfmr5.pdf\n",
            " tfsk1.pdf\n",
            "'Thinking forum.pdf'\n",
            " toaz.info-microelectronic-circuits-by-sedra-smith-7th-editionpdf-pr_25e4a85f6ef2079300ee4d8656758c1c.pdf\n",
            " Track_100.mp3\n",
            " Track_101.mp3\n",
            " Track_102.mp3\n",
            " Track_103.mp3\n",
            " Track_104.mp3\n",
            " Track_105.mp3\n",
            " Track_106.mp3\n",
            " Track_107.mp3\n",
            " Track_108.mp3\n",
            " Track_109.mp3\n",
            " Track_10.mp3\n",
            " Track_110.mp3\n",
            " Track_111.mp3\n",
            " Track_112.mp3\n",
            " Track_113.mp3\n",
            " Track_114.mp3\n",
            " Track_115.mp3\n",
            " Track_116.mp3\n",
            " Track_117.mp3\n",
            " Track_118.mp3\n",
            " Track_119.mp3\n",
            " Track_11.mp3\n",
            " Track_120.mp3\n",
            " Track_121.mp3\n",
            " Track_122.mp3\n",
            " Track_123.mp3\n",
            " Track_124.mp3\n",
            " Track_125.mp3\n",
            " Track_126.mp3\n",
            " Track_127.mp3\n",
            " Track_128.mp3\n",
            " Track_129.mp3\n",
            " Track_12.mp3\n",
            " Track_130.mp3\n",
            " Track_131.mp3\n",
            " Track_132.mp3\n",
            " Track_133.mp3\n",
            " Track_134.mp3\n",
            " Track_135.mp3\n",
            " Track_136.mp3\n",
            " Track_137.mp3\n",
            " Track_138.mp3\n",
            " Track_139.mp3\n",
            " Track_13.mp3\n",
            " Track_140.mp3\n",
            " Track_141.mp3\n",
            " Track_142.mp3\n",
            " Track_143.mp3\n",
            " Track_144.mp3\n",
            " Track_145.mp3\n",
            " Track_146.mp3\n",
            " Track_147.mp3\n",
            " Track_148.mp3\n",
            " Track_149.mp3\n",
            " Track_14.mp3\n",
            " Track_150.mp3\n",
            " Track_151.mp3\n",
            " Track_152.mp3\n",
            " Track_153.mp3\n",
            " Track_154.mp3\n",
            " Track_155.mp3\n",
            " Track_156.mp3\n",
            " Track_157.mp3\n",
            " Track_158.mp3\n",
            " Track_159.mp3\n",
            " Track_15.mp3\n",
            " Track_160.mp3\n",
            " Track_161.mp3\n",
            " Track_162.mp3\n",
            " Track_163.mp3\n",
            " Track_164.mp3\n",
            " Track_165.mp3\n",
            " Track_166.mp3\n",
            " Track_167.mp3\n",
            " Track_168.mp3\n",
            " Track_16.mp3\n",
            " Track_17.mp3\n",
            " Track_18.mp3\n",
            " Track_19.mp3\n",
            " Track_1.mp3\n",
            " Track_20.mp3\n",
            " Track_21.mp3\n",
            " Track_22.mp3\n",
            " Track_23.mp3\n",
            " Track_24.mp3\n",
            " Track_25.mp3\n",
            " Track_26.mp3\n",
            " Track_27.mp3\n",
            " Track_28.mp3\n",
            " Track_29.mp3\n",
            " Track_2.mp3\n",
            " Track_30.mp3\n",
            " Track_31.mp3\n",
            " Track_32.mp3\n",
            " Track_33.mp3\n",
            " Track_34.mp3\n",
            " Track_35.mp3\n",
            " Track_36.mp3\n",
            " Track_37.mp3\n",
            " Track_38.mp3\n",
            " Track_39.mp3\n",
            " Track_3.mp3\n",
            " Track_40.mp3\n",
            " Track_41.mp3\n",
            " Track_42.mp3\n",
            " Track_43.mp3\n",
            " Track_44.mp3\n",
            " Track_45.mp3\n",
            " Track_46.mp3\n",
            " Track_47.mp3\n",
            " Track_48.mp3\n",
            " Track_49.mp3\n",
            " Track_4.mp3\n",
            " Track_50.mp3\n",
            " Track_51.mp3\n",
            " Track_52.mp3\n",
            " Track_53.mp3\n",
            " Track_54.mp3\n",
            " Track_55.mp3\n",
            " Track_56.mp3\n",
            " Track_57.mp3\n",
            " Track_58.mp3\n",
            " Track_59.mp3\n",
            " Track_5.mp3\n",
            " Track_60.mp3\n",
            " Track_61.mp3\n",
            " Track_62.mp3\n",
            " Track_63.mp3\n",
            " Track_64.mp3\n",
            " Track_65.mp3\n",
            " Track_66.mp3\n",
            " Track_67.mp3\n",
            " Track_68.mp3\n",
            " Track_69.mp3\n",
            " Track_6.mp3\n",
            " Track_70.mp3\n",
            " Track_71.mp3\n",
            " Track_72.mp3\n",
            " Track_73.mp3\n",
            " Track_74.mp3\n",
            " Track_75.mp3\n",
            " Track_76.mp3\n",
            " Track_77.mp3\n",
            " Track_78.mp3\n",
            " Track_79.mp3\n",
            " Track_7.mp3\n",
            " Track_80.mp3\n",
            " Track_81.mp3\n",
            " Track_82.mp3\n",
            " Track_83.mp3\n",
            " Track_84.mp3\n",
            " Track_85.mp3\n",
            " Track_86.mp3\n",
            " Track_87.mp3\n",
            " Track_88.mp3\n",
            " Track_89.mp3\n",
            " Track_8.mp3\n",
            " Track_90.mp3\n",
            " Track_91.mp3\n",
            " Track_92.mp3\n",
            " Track_93.mp3\n",
            " Track_94.mp3\n",
            " Track_95.mp3\n",
            " Track_96.mp3\n",
            " Track_97.mp3\n",
            " Track_98.mp3\n",
            " Track_99.mp3\n",
            " Track_9.mp3\n",
            " Untitled0.ipynb\n",
            "'물방울 컴퍼니 UXUI 디자인.ai'\n",
            " v1.pdf\n",
            " v2.pdf\n",
            " v3.pdf\n",
            " v4.pdf\n",
            " VID_45730809_164818_540.mp4\n",
            " VID_45730810_192015_886.mp4\n",
            " wr중급실전2.pdf\n",
            " WR중급실전3.pdf\n",
            " wr중급실전.pdf\n",
            " wt1.pdf\n",
            " wt2.pdf\n",
            " wt3.pdf\n",
            " wt4.pdf\n",
            " wt5.pdf\n",
            " wt6.pdf\n",
            " wt7.pdf\n",
            " wt8.pdf\n",
            " wt9.pdf\n",
            " wtdap1.pdf\n",
            " wtdap2.pdf\n",
            " wtdap3.pdf\n",
            " Y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from bs4 import BeautifulSoup \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import urllib.request\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
        "\n",
        "resolver=tf.distribute.cluster_resolver.TPUClusterResolver('grpc://10.119.43.66:8470')\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "print('=3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfFY2tceTdpW",
        "outputId": "dd16157d-115d-4ed6-8b31-535d650f70c0"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.119.43.66:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.119.43.66:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.119.43.66:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.119.43.66:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "try:\n",
        "  device_name = os.environ['COLAB_TPU_ADDR']\n",
        "  TPU_ADDRESS = 'grpc://' + device_name\n",
        "  print('Found TPU at: {}'.format(TPU_ADDRESS))\n",
        "\n",
        "except KeyError:\n",
        "  print('TPU not found')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSiHtpPFTd3U",
        "outputId": "d44e8742-d8e8-4796-a105-e8bd90106a38"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found TPU at: grpc://10.119.43.66:8470\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary_more.csv\", filename=\"news_summary_more.csv\")\n",
        "data = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')"
      ],
      "metadata": {
        "id": "c3G1wti9Td73"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "wJUa6sPsTd-M",
        "outputId": "f438d895-f72b-474a-89ca-9cd32583614c"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-17b9187a-9f45-4e43-b739-c06a096fc050\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headlines</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n",
              "      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Delhi techie wins free food from Swiggy for on...</td>\n",
              "      <td>Kunal Shah's credit card bill payment platform...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>New Zealand end Rohit Sharma-led India's 12-ma...</td>\n",
              "      <td>New Zealand defeated India by 8 wickets in the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aegon life iTerm insurance plan helps customer...</td>\n",
              "      <td>With Aegon Life iTerm Insurance plan, customer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Have known Hirani for yrs, what if MeToo claim...</td>\n",
              "      <td>Speaking about the sexual harassment allegatio...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-17b9187a-9f45-4e43-b739-c06a096fc050')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-17b9187a-9f45-4e43-b739-c06a096fc050 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-17b9187a-9f45-4e43-b739-c06a096fc050');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                           headlines                                               text\n",
              "0  upGrad learner switches to career in ML & Al w...  Saurav Kant, an alumnus of upGrad and IIIT-B's...\n",
              "1  Delhi techie wins free food from Swiggy for on...  Kunal Shah's credit card bill payment platform...\n",
              "2  New Zealand end Rohit Sharma-led India's 12-ma...  New Zealand defeated India by 8 wickets in the...\n",
              "3  Aegon life iTerm insurance plan helps customer...  With Aegon Life iTerm Insurance plan, customer...\n",
              "4  Have known Hirani for yrs, what if MeToo claim...  Speaking about the sexual harassment allegatio..."
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[['text','headlines']]\n",
        "data.head()\n",
        "\n",
        "#랜덤한 15개 샘플 출력\n",
        "data.sample(15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "FT0a4kLGTeAg",
        "outputId": "06681cd0-4a9c-4d8d-e697-58d85a63d858"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0757efed-b46b-486c-b7f6-3119ffaad153\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>headlines</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>37891</th>\n",
              "      <td>Defending champions Germany started their 2018...</td>\n",
              "      <td>Germany lose their opening game of WC for 1st ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84180</th>\n",
              "      <td>Eight letters written and signed by Albert Ein...</td>\n",
              "      <td>Einstein's letters on God and physics fetch ov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27248</th>\n",
              "      <td>Taiwan on Thursday said it is \"deeply disappoi...</td>\n",
              "      <td>Deeply disappointed: Taiwan on AI renaming it ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14784</th>\n",
              "      <td>Reacting to 18-year-old Prithvi Shaw's 134-run...</td>\n",
              "      <td>Debutant Shaw batted like he has played 50 Tes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9627</th>\n",
              "      <td>Tunnelling startup The Boring Company's CEO El...</td>\n",
              "      <td>Elon Musk launches first store to sell bricks ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49884</th>\n",
              "      <td>According to reports, actress Disha Patani wil...</td>\n",
              "      <td>Disha to recreate Madhuri's 'Ek Do Teen' in Ba...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71364</th>\n",
              "      <td>Jammu and Kashmir Liberation Front chief Yasin...</td>\n",
              "      <td>Kashmiri separatist leader Yasin Malik arreste...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46528</th>\n",
              "      <td>The Enforcement Directorate (ED) on Friday fro...</td>\n",
              "      <td>ED freezes deposits, shares worth Ã¢ÂÂ¹44 cr ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28356</th>\n",
              "      <td>If England and Belgium play out a draw on Thur...</td>\n",
              "      <td>Who will win World Cup's Group G if England, B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83335</th>\n",
              "      <td>Italy and US-based researchers have co-develop...</td>\n",
              "      <td>Optic fibre probe 200 times thinner than human...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29039</th>\n",
              "      <td>English theoretical physicist Peter Higgs, who...</td>\n",
              "      <td>Scientist won Nobel 49 years after predicting ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4119</th>\n",
              "      <td>Delhi Police is going to deploy around 15,000 ...</td>\n",
              "      <td>Delhi to deploy 15k cops to ensure safety on N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82496</th>\n",
              "      <td>Mossad, the National Security and Intelligence...</td>\n",
              "      <td>Israel intelligence agency funds startups maki...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68522</th>\n",
              "      <td>The Delhi High Court has cancelled the issued ...</td>\n",
              "      <td>Delhi HC cancels warrant against AskMe's forme...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88836</th>\n",
              "      <td>Actress Manisha Koirala, while talking about b...</td>\n",
              "      <td>1st reaction was 'I've to play Sanjay's mother...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0757efed-b46b-486c-b7f6-3119ffaad153')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0757efed-b46b-486c-b7f6-3119ffaad153 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0757efed-b46b-486c-b7f6-3119ffaad153');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                    text                                          headlines\n",
              "37891  Defending champions Germany started their 2018...  Germany lose their opening game of WC for 1st ...\n",
              "84180  Eight letters written and signed by Albert Ein...  Einstein's letters on God and physics fetch ov...\n",
              "27248  Taiwan on Thursday said it is \"deeply disappoi...  Deeply disappointed: Taiwan on AI renaming it ...\n",
              "14784  Reacting to 18-year-old Prithvi Shaw's 134-run...  Debutant Shaw batted like he has played 50 Tes...\n",
              "9627   Tunnelling startup The Boring Company's CEO El...  Elon Musk launches first store to sell bricks ...\n",
              "49884  According to reports, actress Disha Patani wil...  Disha to recreate Madhuri's 'Ek Do Teen' in Ba...\n",
              "71364  Jammu and Kashmir Liberation Front chief Yasin...  Kashmiri separatist leader Yasin Malik arreste...\n",
              "46528  The Enforcement Directorate (ED) on Friday fro...  ED freezes deposits, shares worth Ã¢ÂÂ¹44 cr ...\n",
              "28356  If England and Belgium play out a draw on Thur...  Who will win World Cup's Group G if England, B...\n",
              "83335  Italy and US-based researchers have co-develop...  Optic fibre probe 200 times thinner than human...\n",
              "29039  English theoretical physicist Peter Higgs, who...  Scientist won Nobel 49 years after predicting ...\n",
              "4119   Delhi Police is going to deploy around 15,000 ...  Delhi to deploy 15k cops to ensure safety on N...\n",
              "82496  Mossad, the National Security and Intelligence...  Israel intelligence agency funds startups maki...\n",
              "68522  The Delhi High Court has cancelled the issued ...  Delhi HC cancels warrant against AskMe's forme...\n",
              "88836  Actress Manisha Koirala, while talking about b...  1st reaction was 'I've to play Sanjay's mother..."
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Text 열에서 중복을 배제한 유일한 샘플의 수 :', data['text'].nunique())\n",
        "print('Summary 열에서 중복을 배제한 유일한 샘플의 수 :', data['headlines'].nunique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZOGBZ0NXpaI",
        "outputId": "53076fdd-512d-42c2-d037-f2e1257225ee"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 열에서 중복을 배제한 유일한 샘플의 수 : 98360\n",
            "Summary 열에서 중복을 배제한 유일한 샘플의 수 : 98280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inplace=True 를 설정하면 DataFrame 타입 값을 return 하지 않고 data 내부를 직접적으로 바꿉니다\n",
        "data.drop_duplicates(subset = ['text'], inplace=True)\n",
        "print('전체 샘플수 :', (len(data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQC4DhQeXsPQ",
        "outputId": "dcf63347-ec47-4776-aed6-da80f6038bd2"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플수 : 98360\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPqz9-fXXufe",
        "outputId": "d22b7360-e43d-40f8-f2d5-5c0280fdb417"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text         0\n",
            "headlines    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.dropna(axis=0, inplace=True)\n",
        "print('전체 샘플수 :', (len(data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcvHi2xPXwvb",
        "outputId": "feb7a785-a0ee-4a59-de1c-cbeaa154fa05"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플수 : 98360\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "\n",
        "print(\"정규화 사전의 수: \", len(contractions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfvKTW3VX0Im",
        "outputId": "ca485a89-12e4-4829-a057-34f49aee6484"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정규화 사전의 수:  120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('불용어 개수 :', len(stopwords.words('english') ))\n",
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDnljpPbX0Ks",
        "outputId": "4f0e9452-59d6-4ffb-8593-cdbc98887ad9"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 개수 : 179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리 함수\n",
        "def preprocess_sentence(sentence, remove_stopwords=True):\n",
        "    sentence = sentence.lower() # 텍스트 소문자화\n",
        "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n",
        "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
        "    sentence = re.sub('\"','', sentence) # 쌍따옴표 \" 제거\n",
        "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
        "    sentence = re.sub(r\"'s\\b\",\"\", sentence) # 소유격 제거. Ex) roland's -> roland\n",
        "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
        "    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n",
        "    \n",
        "    # 불용어 제거 (Text)\n",
        "    if remove_stopwords:\n",
        "        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)\n",
        "    # 불용어 미제거 (Summary)\n",
        "    else:\n",
        "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
        "    return tokens\n",
        "print('=3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8caxe_wKX0My",
        "outputId": "c4c385f7-67fc-4d93-d6bd-512d381b8270"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_text = 'Everything I bought was great, infact I ordered twice and the third ordered was<br />for my mother and father.'\n",
        "temp_summary = 'Great way to start (or finish) the day!!!'\n",
        "\n",
        "print(\"text: \", preprocess_sentence(temp_text))\n",
        "print(\"summary:\", preprocess_sentence(temp_summary, False))  # 불용어를 제거하지 않습니다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baE7Cm1FX0O2",
        "outputId": "c76281a5-9af8-4a1d-970f-6b9f328337f8"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text:  everything bought great infact ordered twice third ordered wasfor mother father\n",
            "summary: great way to start the day\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text = []\n",
        "# 전체 Text 데이터에 대한 전처리 : 10분 이상 시간이 걸릴 수 있습니다. \n",
        "for s in data['text']:\n",
        "    clean_text.append(preprocess_sentence(s))\n",
        "\n",
        "# 전처리 후 출력\n",
        "print(\"Text 전처리 후 결과: \", clean_text[:5])"
      ],
      "metadata": {
        "id": "tfHaYKNLX0Q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_summary = []\n",
        "# 전체 Summary 데이터에 대한 전처리 : 5분 이상 시간이 걸릴 수 있습니다. \n",
        "for s in data['headlines']:\n",
        "    clean_summary.append(preprocess_sentence(s, False))\n",
        "\n",
        "print(\"Summary 전처리 후 결과: \", clean_summary[:5])"
      ],
      "metadata": {
        "id": "XwMoXmSaX0TE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['text'] = clean_text\n",
        "data['headlines'] = clean_summary\n",
        "\n",
        "# 빈 값을 Null 값으로 변환\n",
        "data.replace('', np.nan, inplace=True)\n",
        "print('=3')"
      ],
      "metadata": {
        "id": "g72TWhGlX_3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "N3tw-pwmX_6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.dropna(axis=0, inplace=True)\n",
        "print('전체 샘플수 :', (len(data)))"
      ],
      "metadata": {
        "id": "0kqN_rhlX_8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 길이 분포 출력\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text_len = [len(s.split()) for s in data['text']]\n",
        "headlines_len = [len(s.split()) for s in data['headlines']]\n",
        "\n",
        "print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n",
        "print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
        "print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n",
        "print('요약의 최소 길이 : {}'.format(np.min(headlines_len)))\n",
        "print('요약의 최대 길이 : {}'.format(np.max(headlines_len)))\n",
        "print('요약의 평균 길이 : {}'.format(np.mean(headlines_len)))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.boxplot(text_len)\n",
        "plt.title('Text')\n",
        "plt.subplot(1,2,2)\n",
        "plt.boxplot(headlines_len)\n",
        "plt.title('Summary')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.title('Text')\n",
        "plt.hist(text_len, bins = 40)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()\n",
        "\n",
        "plt.title('Summary')\n",
        "plt.hist(headlines_len, bins = 40)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CpFBRLMaX_-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_max_len = 50\n",
        "summary_max_len = 8\n",
        "print('=3')"
      ],
      "metadata": {
        "id": "mWXN-2WvYABa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def below_threshold_len(max_len, nested_list):\n",
        "  cnt = 0\n",
        "  for s in nested_list:\n",
        "    if(len(s.split()) <= max_len):\n",
        "        cnt = cnt + 1\n",
        "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))\n",
        "print('=3')"
      ],
      "metadata": {
        "id": "egmX8XpNYAD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "below_threshold_len(text_max_len, data['text'])\n",
        "below_threshold_len(summary_max_len,  data['headlines'])"
      ],
      "metadata": {
        "id": "Db8g-EjBYAGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[data['text'].apply(lambda x: len(x.split()) <= text_max_len)]\n",
        "data = data[data['headlines'].apply(lambda x: len(x.split()) <= summary_max_len)]\n",
        "print('전체 샘플수 :', (len(data)))"
      ],
      "metadata": {
        "id": "016lwX-AYAJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 요약 데이터에는 시작 토큰과 종료 토큰을 추가한다.\n",
        "data['decoder_input'] = data['headlines'].apply(lambda x : 'sostoken '+ x)\n",
        "data['decoder_target'] = data['headlines'].apply(lambda x : x + ' eostoken')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "Zv8Bj0SNYALT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = np.array(data['text']) # 인코더의 입력\n",
        "decoder_input = np.array(data['decoder_input']) # 디코더의 입력\n",
        "decoder_target = np.array(data['decoder_target']) # 디코더의 레이블\n",
        "print('=3')"
      ],
      "metadata": {
        "id": "6wXYiC8rYANq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices = np.arange(encoder_input.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "print(indices)"
      ],
      "metadata": {
        "id": "gm8naTWBYAQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = encoder_input[indices]\n",
        "decoder_input = decoder_input[indices]\n",
        "decoder_target = decoder_target[indices]\n",
        "print('=3')"
      ],
      "metadata": {
        "id": "w4OGhHLWYASJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_of_val = int(len(encoder_input)*0.2)\n",
        "print('테스트 데이터의 수 :', n_of_val)"
      ],
      "metadata": {
        "id": "ejcKFaQhYAUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_train = encoder_input[:-n_of_val]\n",
        "decoder_input_train = decoder_input[:-n_of_val]\n",
        "decoder_target_train = decoder_target[:-n_of_val]\n",
        "\n",
        "encoder_input_test = encoder_input[-n_of_val:]\n",
        "decoder_input_test = decoder_input[-n_of_val:]\n",
        "decoder_target_test = decoder_target[-n_of_val:]\n",
        "\n",
        "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
        "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
        "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
        "print('테스트 레이블의 개수 :', len(decoder_input_test))"
      ],
      "metadata": {
        "id": "oWRJtVnhYAW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_tokenizer = Tokenizer() # 토크나이저 정의\n",
        "src_tokenizer.fit_on_texts(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성\n",
        "print('=3')"
      ],
      "metadata": {
        "id": "G40SfS_fYAZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 7\n",
        "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in src_tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
        "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
      ],
      "metadata": {
        "id": "MITv52MpYAbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab = 8000\n",
        "src_tokenizer = Tokenizer(num_words=src_vocab) # 단어 집합의 크기를 8,000으로 제한\n",
        "src_tokenizer.fit_on_texts(encoder_input_train) # 단어 집합 재생성\n",
        "print('=3')"
      ],
      "metadata": {
        "id": "6O5QifajbxEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
        "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train) \n",
        "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
        "\n",
        "# 잘 진행되었는지 샘플 출력\n",
        "print(encoder_input_train[:3])"
      ],
      "metadata": {
        "id": "JfZPhi1XbxHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tar_tokenizer = Tokenizer()\n",
        "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
        "print('=3')"
      ],
      "metadata": {
        "id": "IS-CIH6tbxJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 6\n",
        "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tar_tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
        "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
      ],
      "metadata": {
        "id": "YqNa92EobxLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tar_vocab = 2000\n",
        "tar_tokenizer = Tokenizer(num_words=tar_vocab) \n",
        "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
        "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
        "\n",
        "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
        "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train) \n",
        "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
        "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
        "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
        "\n",
        "# 잘 변환되었는지 확인\n",
        "print('input')\n",
        "print('input ',decoder_input_train[:5])\n",
        "print('target')\n",
        "print('decoder ',decoder_target_train[:5])"
      ],
      "metadata": {
        "id": "FFiNsGGobxNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
        "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
        "\n",
        "print('삭제할 훈련 데이터의 개수 :', len(drop_train))\n",
        "print('삭제할 테스트 데이터의 개수 :', len(drop_test))\n",
        "\n",
        "encoder_input_train = [sentence for index, sentence in enumerate(encoder_input_train) if index not in drop_train]\n",
        "decoder_input_train = [sentence for index, sentence in enumerate(decoder_input_train) if index not in drop_train]\n",
        "decoder_target_train = [sentence for index, sentence in enumerate(decoder_target_train) if index not in drop_train]\n",
        "\n",
        "encoder_input_test = [sentence for index, sentence in enumerate(encoder_input_test) if index not in drop_test]\n",
        "decoder_input_test = [sentence for index, sentence in enumerate(decoder_input_test) if index not in drop_test]\n",
        "decoder_target_test = [sentence for index, sentence in enumerate(decoder_target_test) if index not in drop_test]\n",
        "\n",
        "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
        "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
        "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
        "print('테스트 레이블의 개수 :', len(decoder_input_test))"
      ],
      "metadata": {
        "id": "oTDL1kDdbxPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_train = pad_sequences(encoder_input_train, maxlen=text_max_len, padding='post')\n",
        "encoder_input_test = pad_sequences(encoder_input_test, maxlen=text_max_len, padding='post')\n",
        "decoder_input_train = pad_sequences(decoder_input_train, maxlen=summary_max_len, padding='post')\n",
        "decoder_target_train = pad_sequences(decoder_target_train, maxlen=summary_max_len, padding='post')\n",
        "decoder_input_test = pad_sequences(decoder_input_test, maxlen=summary_max_len, padding='post')\n",
        "decoder_target_test = pad_sequences(decoder_target_test, maxlen=summary_max_len, padding='post')\n",
        "print('=3')"
      ],
      "metadata": {
        "id": "h5EsJrIIYAdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import GaussianNoise\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, LeakyReLU\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.layers import Activation, Dense\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import activations\n",
        "\n",
        "# 인코더 설계 시작\n",
        "embedding_dim = 128\n",
        "hidden_size = 256\n",
        "\n",
        "# 인코더\n",
        "encoder_inputs = Input(shape=(text_max_len,))\n",
        "#encoder_inputs = GaussianNoise(stddev=0.08)(encoder_inputs)\n",
        "# 인코더의 임베딩 층\n",
        "enc_emb2 = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
        "#enc_emb2 = GaussianNoise(stddev=0.1)(enc_emb)\n",
        "# 인코더의 LSTM 1\n",
        "encoder_lstm1 = LSTM(hidden_size, activation= 'tanh' ,return_sequences=True, return_state=True ,dropout = 0.38, recurrent_dropout = 0.42)\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb2)\n",
        "#state_c1 = GaussianNoise(stddev=0.01)(state_c1)\n",
        "# 인코더의 LSTM 2\n",
        "encoder_lstm2 = LSTM(hidden_size, activation= 'tanh', return_sequences=True, return_state=True, dropout=0.38, recurrent_dropout=0.47)\n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "#state_c2 = GaussianNoise(stddev=0.01)(state_c2)\n",
        "# 인코더의 LSTM 3\n",
        "encoder_lstm3 = LSTM(hidden_size, activation= 'tanh', return_state=True, return_sequences=True, dropout=0.38, recurrent_dropout=0.47)\n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)"
      ],
      "metadata": {
        "id": "WQi-LE6PYAf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import noise layer\n",
        "from keras.layers import GaussianNoise\n",
        "# 디코더 설계\n",
        "#tar_vocab3 = GaussianNoise(stddev=0.09)(tar_vocab)\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "# 디코더의 임베딩 층\n",
        "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
        "\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "#dec_emb2 = GaussianNoise(stddev=0.09)(dec_emb)\n",
        "# 디코더의 LSTM\n",
        "state_h2 = GaussianNoise(stddev=0.09)(state_h)\n",
        "decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h2, state_c])"
      ],
      "metadata": {
        "id": "ATUuqIdKYAij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 디코더의 출력층\n",
        "# print(tar_vocab.shape)\n",
        "#tar_vocab2 = GaussianNoise(stddev=0.09)(tar_vocab)\n",
        "#tar_vocab2 = GaussianNoise(stddev=0.09)(tar_vocab)\n",
        "#print(tar_vocab.shape)#'int' object has no attribute 'shape'\n",
        "\n",
        "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
        "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) #위에서 나온 디코더 아웃풋,즉 배치의 샘플수를 softmax layer에 입력으로 넣는다.\n",
        "\n",
        "\n",
        "# 모델 정의\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "DiCUsXtpYAk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import AdditiveAttention\n",
        "\n",
        "# 어텐션 층(어텐션 함수)\n",
        "attn_layer = AdditiveAttention(name='attention_layer')\n",
        "\n",
        "# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
        "attn_out = attn_layer([decoder_outputs, encoder_outputs])\n",
        "\n",
        "\n",
        "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "# 디코더의 출력층\n",
        "\n",
        "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
        "\n",
        "#decoder_softmax_outputs = Activation('tanh')(decoder_concat_input) #여기 도전해본 코드가 있습니다. tanh 레이어를 추가시켰습니다.\n",
        "\n",
        "decoder_softmax_outputs2 =decoder_softmax_layer(decoder_softmax_outputs)\n",
        "# 모델 정의\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs2)\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "feyik65sYAnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():# 인코더\n",
        "    encoder_inputs = Input(shape=(text_max_len,))\n",
        "    #encoder_inputs = GaussianNoise(stddev=0.08)(encoder_inputs)\n",
        "    # 인코더의 임베딩 층\n",
        "    enc_emb2 = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
        "    #enc_emb2 = GaussianNoise(stddev=0.1)(enc_emb)\n",
        "    # 인코더의 LSTM 1\n",
        "    encoder_lstm1 = LSTM(hidden_size, activation= 'tanh' ,return_sequences=True, return_state=True ,dropout = 0.38, recurrent_dropout = 0.42)\n",
        "    encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb2)\n",
        "    #state_c1 = GaussianNoise(stddev=0.01)(state_c1)\n",
        "    # 인코더의 LSTM 2\n",
        "    encoder_lstm2 = LSTM(hidden_size, activation= 'tanh', return_sequences=True, return_state=True, dropout=0.38, recurrent_dropout=0.47)\n",
        "    encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "    #state_c2 = GaussianNoise(stddev=0.01)(state_c2)\n",
        "    # 인코더의 LSTM 3\n",
        "    encoder_lstm3 = LSTM(hidden_size, activation= 'tanh', return_state=True, return_sequences=True, dropout=0.38, recurrent_dropout=0.47)\n",
        "    encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
        "\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "    # 디코더의 임베딩 층\n",
        "    dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
        "\n",
        "    dec_emb = dec_emb_layer(decoder_inputs)\n",
        "    #dec_emb2 = GaussianNoise(stddev=0.09)(dec_emb)\n",
        "    # 디코더의 LSTM\n",
        "    #state_h2 = GaussianNoise(stddev=0.09)(state_h)#시도해본 코드 \n",
        "    decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
        "    decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
        "\n",
        "    decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
        "    decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) #위에서 나온 디코더 아웃풋,즉 배치의 샘플수를 softmax layer에 입력으로 넣는다.\n",
        "    # 어텐션 층(어텐션 함수)\n",
        "    attn_layer = AdditiveAttention(name='attention_layer')\n",
        "\n",
        "    # 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
        "    attn_out = attn_layer([decoder_outputs, encoder_outputs])\n",
        "\n",
        "\n",
        "    # 어텐션의 결과와 디코더의 hidden state들을 연결\n",
        "    decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "    # 디코더의 출력층\n",
        "\n",
        "    decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
        "\n",
        "    #decoder_softmax_outputs = Activation('tanh')(decoder_concat_input) #여기 도전해본 코드가 있습니다. tanh 레이어를 추가시켰습니다.\n",
        "\n",
        "    decoder_softmax_outputs2 =decoder_softmax_layer(decoder_softmax_outputs)\n",
        "    # 모델 정의\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs2)\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "    es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
        "    history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
        "                validation_data=([encoder_input_test, decoder_input_test], decoder_target_test), \\\n",
        "                batch_size=256, callbacks=[es], epochs=50)"
      ],
      "metadata": {
        "id": "iqzoqEhXfkvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ddJTsaOhYAsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
        "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
        "tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음\n",
        "\n",
        "print('=3')"
      ],
      "metadata": {
        "id": "LFb5cjg_YAug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더 설계\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# 이전 시점의 상태들을 저장하는 텐서\n",
        "decoder_state_input_h = Input(shape=(hidden_size,))\n",
        "decoder_state_input_c = Input(shape=(hidden_size,))\n",
        "\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
        "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "print('=3')"
      ],
      "metadata": {
        "id": "qJUNKzz3YAw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 어텐션 함수\n",
        "decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
        "attn_out_inf = attn_layer([decoder_outputs2, decoder_hidden_state_input])\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# 디코더의 출력층\n",
        "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n",
        "\n",
        "# 최종 디코더 모델\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs2] + [state_h2, state_c2])\n",
        "\n",
        "print('=3')"
      ],
      "metadata": {
        "id": "ptx-My1edxCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # 입력으로부터 인코더의 상태를 얻음\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "\n",
        "     # <SOS>에 해당하는 토큰 생성\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = tar_word_to_index['sostoken']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n",
        "\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = tar_index_to_word[sampled_token_index]\n",
        "\n",
        "        if (sampled_token!='eostoken'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "        #  <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
        "        if (sampled_token == 'eostoken'  or len(decoded_sentence.split()) >= (summary_max_len-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # 길이가 1인 타겟 시퀀스를 업데이트\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # 상태를 업데이트 합니다.\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence\n",
        "print('=3')"
      ],
      "metadata": {
        "id": "h6D9IMXydxEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_train = encoder_input[:-n_of_val]\n",
        "decoder_input_train = decoder_input[:-n_of_val]\n",
        "decoder_target_train = decoder_target[:-n_of_val]\n",
        "\n",
        "encoder_input_test = encoder_input[-n_of_val:]\n",
        "decoder_input_test = decoder_input[-n_of_val:]\n",
        "decoder_target_test = decoder_target[-n_of_val:]\n",
        "\n",
        "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
        "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
        "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
        "print('테스트 레이블의 개수 :', len(decoder_input_test))\n",
        "\n",
        "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq2text(input_seq):\n",
        "    temp=''\n",
        "    for i in input_seq:\n",
        "        if (i!=0):\n",
        "            temp = temp + src_index_to_word[i]+' '\n",
        "    return temp\n",
        "\n",
        "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq2summary(input_seq):\n",
        "    temp=''\n",
        "    for i in input_seq:\n",
        "        if ((i!=0 and i!=tar_word_to_index['sostoken']) and i!=tar_word_to_index['eostoken']):\n",
        "            temp = temp + tar_index_to_word[i] + ' '\n",
        "    return temp\n",
        "\n",
        "print('=3')\n",
        "\n",
        "\n",
        "#encoder_input_test =encoder_input_test[:2000]\n",
        "# #encoder_input_test2 =encoder_input_test2[:2000]\n",
        "# print(type(encoder_input_test2))\n",
        "# eit = np.array(encoder_input_test2)\n",
        "# eit2 = eit.reshape(-1,2000)\n",
        "# print(type(eit2))\n",
        "# print(eit2)\n",
        "# print(eit)\n",
        "# print(encoder_input_test2)\n",
        "# print(encoder_input_test1)"
      ],
      "metadata": {
        "id": "QmaF6dQhdxGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(encoder_input_test.dtype)\n",
        "# print(type(encoder_input_test))\n",
        "# encoder_input_test1 = encoder_input_test.reshape(-1,)\n",
        "# encoder_input_test2 = np.array(encoder_input_test1, dtype=np.int32) \n",
        "# print(encoder_input_test2.shape)\n",
        "# encoder_input_test.reshape(None,)\n",
        "\n",
        "for i in range(50, 100):\n",
        "    print(\"원문 :\", seq2text(encoder_input_test[i]))\n",
        "    print(\"실제 요약 :\", seq2summary(decoder_input_test[i]))\n",
        "    print(\"예측 요약 :\", decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "h6nHP5ETdxJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from summa.summarizer import summarize"
      ],
      "metadata": {
        "id": "-e3jYSuHdxLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = requests.get('http://rare-technologies.com/the_matrix_synopsis.txt').text"
      ],
      "metadata": {
        "id": "z4zWKUwadxNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1500])"
      ],
      "metadata": {
        "id": "RjWyNbzwdxPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Summary:')\n",
        "print(summarize(text, ratio=0.005))"
      ],
      "metadata": {
        "id": "VQkJAu8XdxRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Summary:')\n",
        "print(summarize(text, ratio=0.005, split=True))"
      ],
      "metadata": {
        "id": "gfXPEVkydxUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Summary:')\n",
        "print(summarize(text, words=50))"
      ],
      "metadata": {
        "id": "JMAebtWhdxWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aHICJrGOdxYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Y5L3UA1Zdxaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Z8aJv0wudxdH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}